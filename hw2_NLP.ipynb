{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUotKHRULVPD"
      },
      "source": [
        "# Инструменты для работы с языком "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba5Z02VLVPK"
      },
      "source": [
        "## Задача: классификация твитов по тональности\n",
        "\n",
        "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
        "\n",
        "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zHB70n5LVPN",
        "outputId": "5da17616-30b1-4f96-86d5-4411738ee648",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-05 04:42:33--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2022-06-05 04:42:33--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com/cd/0/inline/BmngfAHl-7sw0dXwvuh0wZdmdg1L7OX3sWZZW8LO0Dyz3DusdIP6nW52YhzkAibS4ixgSdMC_LrDQHHTcbCsa90fs_Ho1O8uIupjPkctmLJ_Lui0wA7DWYEYFYzX2wf-TAFzMyOiP8edOboLsZSr2yyGMDg5_dgkDNOWlD832a1wFA/file# [following]\n",
            "--2022-06-05 04:42:33--  https://uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com/cd/0/inline/BmngfAHl-7sw0dXwvuh0wZdmdg1L7OX3sWZZW8LO0Dyz3DusdIP6nW52YhzkAibS4ixgSdMC_LrDQHHTcbCsa90fs_Ho1O8uIupjPkctmLJ_Lui0wA7DWYEYFYzX2wf-TAFzMyOiP8edOboLsZSr2yyGMDg5_dgkDNOWlD832a1wFA/file\n",
            "Resolving uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com (uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com (uc37bc210cade896f549488a0c91.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  64.1MB/s    in 0.4s    \n",
            "\n",
            "2022-06-05 04:42:34 (64.1 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n",
            "--2022-06-05 04:42:34--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2022-06-05 04:42:35--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com/cd/0/inline/Bmlu9KmvjR_bUP4q26FGkJ_-j_JVQENqyvbIv8SuuybyCzFVJyinpBmvWari7EzTKYek7h2ormsrvn1K0g2V1o95bj6sKcUHt5Z2j3ul5SExtDlyYGhR2qipo15bgQZ5qG-HOzEaQUuj6a7rl83-hDuJkqAj2WleL-T0_J5EIQUXPw/file# [following]\n",
            "--2022-06-05 04:42:35--  https://uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com/cd/0/inline/Bmlu9KmvjR_bUP4q26FGkJ_-j_JVQENqyvbIv8SuuybyCzFVJyinpBmvWari7EzTKYek7h2ormsrvn1K0g2V1o95bj6sKcUHt5Z2j3ul5SExtDlyYGhR2qipo15bgQZ5qG-HOzEaQUuj6a7rl83-hDuJkqAj2WleL-T0_J5EIQUXPw/file\n",
            "Resolving uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com (uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com (uc36d93fda9385c86529059485fe.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M  63.7MB/s    in 0.4s    \n",
            "\n",
            "2022-06-05 04:42:36 (63.7 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J5YiZNCPLVPe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DFLtXAZ-LVPq"
      },
      "outputs": [],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j1AEISlBLVP0",
        "outputId": "a52d4657-c939-4a77-a445-f24f3000af18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text     label\n",
              "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
              "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
              "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
              "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
              "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f494a3d-e26f-4eaa-9d8c-2b47eabd9b6b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111918</th>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111919</th>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111920</th>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111921</th>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111922</th>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f494a3d-e26f-4eaa-9d8c-2b47eabd9b6b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f494a3d-e26f-4eaa-9d8c-2b47eabd9b6b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f494a3d-e26f-4eaa-9d8c-2b47eabd9b6b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCIRsoS-fQm4",
        "outputId": "def7f63d-7729-4a5a-d783-9d628f51ee63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    114911\n",
              "negative    111923\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZWta7oDgLVP8"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4v7WR8lfcvg",
        "outputId": "b72ca931-beb9-485b-a108-8fde1f700da0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((170125,), (56709,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAapBC7VLVQC"
      },
      "source": [
        "## Baseline: классификация необработанных n-грамм\n",
        "\n",
        "### Векторизаторы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-AvVt8XLVQD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSuoVoxcLVQI"
      },
      "source": [
        "Что такое n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeNA7732LVQJ"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeApDOmrLVQN",
        "outputId": "41e22a43-4968-4894-a0fc-894cf6d1fabe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = 'Если б мне платили каждый раз'.split()\n",
        "list(ngrams(sent, 1)) # униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPAS0fS-LVQQ",
        "outputId": "a4cd75cc-9c51-4dad-965c-fc59f618113e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 2)) # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77jmVPhLVQU",
        "outputId": "631c6f01-d9e5-4968-96a5-3e34104b6dd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 3)) # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xXTBrGELVQX",
        "outputId": "274b0a4e-3343-444f-ac38-f7029ab9d8f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sent, 5)) # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHGJBEm-LVQb"
      },
      "source": [
        "Самый простой способ извлечь фичи из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает простую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMqZFBTgLVQb"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZkpqVtILVQe"
      },
      "source": [
        "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
        "\n",
        "В vec.vocabulary_ лежит словарь: мэппинг слов к их индексам:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWRtOSzKLVQf",
        "outputId": "8bcf5c73-07e9-4a05-8dd8-f69eb8c41968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('нормальных', 168924),\n",
              " ('людей', 154543),\n",
              " ('сегодня', 207830),\n",
              " ('последний', 189888),\n",
              " ('учебный', 229392),\n",
              " ('день', 124531),\n",
              " ('нас', 164054),\n",
              " ('завтра', 132996),\n",
              " ('ну', 169257),\n",
              " ('что', 237614)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(vec.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Ws2gwXf1BP",
        "outputId": "65dfa701-b7e2-4e52-8ae9-565b0d895954"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "243755"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vec.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkmX3iBbLVQi",
        "outputId": "2383aa93-8a3d-43c8-cab2-6a24d6b382c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=42)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ8q5_59LVQm",
        "outputId": "5024c7b2-2f20-4c92-b47e-7227752f13e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.76      0.76     28095\n",
            "    positive       0.77      0.77      0.77     28614\n",
            "\n",
            "    accuracy                           0.77     56709\n",
            "   macro avg       0.77      0.77      0.77     56709\n",
            "weighted avg       0.77      0.77      0.77     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhgaYgqLVQp"
      },
      "source": [
        "Попробуем сделать то же самое для триграмм:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPWXlh6ALVQq",
        "outputId": "e75d1ca0-8118-416a-b914-d2d463ae02a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.57      0.68     41314\n",
            "    positive       0.39      0.72      0.50     15395\n",
            "\n",
            "    accuracy                           0.61     56709\n",
            "   macro avg       0.62      0.65      0.59     56709\n",
            "weighted avg       0.72      0.61      0.63     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnfyJkzTLVQu"
      },
      "source": [
        "(как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTamxPrHg4pC"
      },
      "source": [
        "Судя по всему, в корпусе есть триграммы, которые очень хорошо классифицируют определенный класс негативных отзывов, из-за чего получился высокий precision по негативному классу. Но на это типе негативных отзывов произошло переобучение, и-за чего получился низкий recall.\n",
        "А у позитивного класса наоборот триграммы не позволяют качественно классифицировать такие твиты и на позитивном классе у нас недообучение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJABxhalLVQu"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LJES2s-LVQv"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "TF (term frequency) – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
        "\n",
        "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
        "\n",
        "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "`t` -- слово (term), `D` -- коллекция документов\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF_(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Сакральный смысл – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmEcRD28LVQ0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWLhMl9xLVQ3",
        "outputId": "d468ff24-4ec8-4475-840f-a4dc9600a085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.77      0.75     26730\n",
            "    positive       0.78      0.75      0.77     29979\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTODTRnKLVQ6"
      },
      "source": [
        "В этот раз получилось хуже :( Вернёмся к `CountVectorizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8v9Scpn9Y0M"
      },
      "source": [
        "## PMI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRqLcSY0etj"
      },
      "source": [
        "Можно оценить взаимосвязь слов в корпусе и понять, какие биграммы наиболее часто встречаются в тексте. Для этого можно использовать метрику PMI (Pointwise Mutual Information) - поточечная взаимная информация. Метрика PMI для двух слов вычисляется по формуле:\n",
        "\n",
        "$$pmi(x; y) = log \\frac{p(x,y)}{p(x)p(y)} $$\n",
        "\n",
        "Здесь p(y|x) - вероятность встретить слово $y$ после $x$, $p(y)$ - вероятность встретить слово $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXgDwf6W6Kk5"
      },
      "source": [
        "Оценим важность биграмм в нашем обучающем корпусе."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKmiOEaW53F9",
        "outputId": "3ddbfbe0-c5b4-4ab9-b562-47e3ee0ee9d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
            "[('***ахах', 'Ксююю'), ('***сегодня', 'ироска'), ('**азааз', 'чукрк'), ('*FUCK', 'ЛОГИК*'), ('*__________________*', 'вообщеееееееее'), ('*blog', 'Трансформация'), ('*facepalm.jpg*', 'Ебанные'), ('*happy', 'tree*'), ('*linux', '*arch'), ('*might*', 'record'), ('*Кукэева', 'минем**Лучшая'), ('*Мне', 'отвёртку'), ('*ПОТЕКЛИ', 'СЛЮНИ*'), ('*СЛЕЗЫ', 'СЛЕЗЫ*'), ('*Ургант*', '-Давайте'), ('*аалгах', 'гээх'), ('*барабанная', 'дробь*'), ('*без', 'намёков*'), ('*белая', 'зависть*'), ('*вот', 'зараза*'), ('*вчера', 'перечитывал'), ('*гениальный', 'вопрос*'), ('*для', 'тех.кого'), ('*довольная', 'ангарская'), ('*ещё', 'глаток*'), ('*зачоркнуто*', 'мявкнет'), ('*ирный', 'у*людок'), ('*коварная', 'мысль*'), ('*крепко', 'обнял*'), ('*ленивая', 'жопа*'), ('*мастер', 'отмазок*'), ('*меня', 'несет*'), ('*намеренно', 'избегает'), ('*нормальным', 'голосом*Лен'), ('*обнимаю', 'монитор*'), ('*опустил', 'голову*'), ('*палец', 'вверх*'), ('*плакаю', 'ностальжы*'), ('*поймал', 'одеялком*'), ('*пока', 'что*'), ('*положил', 'книгу*'), ('*протягивает', 'цветочки*'), ('*сатанинский', 'хохот*'), ('*свои', 'мысли*'), ('*сделал', 'царский'), ('*сладких', 'чуу'), ('*смотрю', 'клип*'), ('*такая', 'милая:3333авввв'), ('*тип', 'я*'), ('*тихим', 'голосом*'), ('*фууф', 'отстрелялись*'), ('*хитрый', 'взгляд*'), ('*щенячьи', 'глазки*'), ('+1239', '728'), ('+375447167151', 'звоги'), ('+живіт', 'болить.ну'), (',4', 'запирайте'), (',Дела', 'рез'), (',как', 'додики'), ('-*..как', 'делиФФки'), ('-..', '-бл'), ('-/////', 'прбрм-прбрм'), ('-10,11', 'болсо'), ('-163', '-КРАСНЫЙ'), ('-165', '-СИНИЙ'), ('-29..', 'ПФ'), ('-700', 'рублей.-А'), ('-800', 'нахууй'), ('-Dдаа..', 'Встречала'), ('-АХАХАХАХ', 'ЮБКУ'), ('-АХАХАХАХХАХАХАХАХАХХА', '-АХАХАХХАХАХАХАХАХ'), ('-Алина', '-Синие'), ('-Аха', 'спетросянил'), ('-ВАХАХАХА', 'СТИПЕНДИЯ'), ('-Вам', 'завернуть'), ('-Весело', 'кншн:3'), ('-Восьмигрудый', 'трипи*дат'), ('-Время', 'эмокора'), ('-Выздоравливай', 'педрилк'), ('-ГНИДОТА', '-Над'), ('-ДЕТЕЙ', 'НАКРЫЛО'), ('-ДОВАЙТИ', 'АЛДСКУЛ'), ('-Дирол', 'Сенсес'), ('-Домашка', '-кл.час'), ('-ЖРАТЬ', 'БАРАНКИ'), ('-ЗАШЛА', 'ОДЕЛА'), ('-Зелено-карие', '-Киллджой'), ('-КРАСНЫЙ', '-ЧЕРНЫЕ'), ('-Керем', 'севгили'), ('-Киллджой', '-Котик'), ('-ЛЮБОВЬ', '*Ургант*'), ('-Мамаааа', 'поправь'), ('-Мг..дядька', 'ахуенный..'), ('-НА', 'РЕАЛЬНЫХ'), ('-НАЧИНАЕТ', 'БЕСИТЬ'), ('-Ну..Идёт', 'Баранов'), ('-ОНИ', 'СТОЯТ'), ('-Олесь', '-Пошёл'), ('-Оооох', 'ебать..'), ('-Песня', 'грусная=*-')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import collocations \n",
        "nltk.download('genesis')\n",
        "\n",
        "print(type(nltk.corpus.genesis.words('english-web.txt')))\n",
        "bigram_measures = collocations.BigramAssocMeasures()\n",
        "# bigram_finder.apply_freq_filter(5)\n",
        "bigram_finder = collocations.BigramCollocationFinder.from_documents([nltk.word_tokenize(x) for x in x_train])\n",
        "bigrams = bigram_finder.nbest(bigram_measures.pmi, 100)\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h4Cq1PUTTc-"
      },
      "source": [
        "Можно рассмотреть другие метрики оценки важности биграмм, например, метрику правдоподобия (подробнее про вычисление метрики можно посмотреть [здесь (пункт 5.3.4)](http://www.corpus.unam.mx/cursoenah/ManningSchutze_1999_FoundationsofStatisticalNaturalLanguageProcessing.pdf):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTOJg4KoOo84",
        "outputId": "ba9b6889-1a1c-4d21-943c-6944d7891b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('(', '('), ('RT', '@'), (')', ')'), ('http', ':'), ('!', '!'), (':', 'D'), ('у', 'меня'), (':', '('), (',', 'а'), (',', 'что'), (',', 'но'), (')', 'http'), ('у', 'нас'), ('(', ','), (':', ')'), (',', '('), ('?', '?'), ('не', 'могу'), (':', '-'), (',', ')'), (':', '*'), (')', ','), (',', ':'), ('@', '('), (',', ','), (':', ','), ('(', ':'), ('@', ','), ('&', 'lt'), ('со', 'мной'), ('@', ':'), ('gt', ';'), ('(', '@'), (':', ':'), (';', ')'), (')', ':'), ('новый', 'год'), (',', '@'), ('не', 'знаю'), ('@', '@'), ('а', 'я'), (',', 'когда'), ('сих', 'пор'), ('потому', 'что'), ('У', 'меня'), ('lt', ';'), ('&', 'gt'), ('у', 'тебя'), (';', '('), ('все', 'равно'), ('с', 'тобой'), ('&', 'amp'), ('в', 'школу'), (',', 'как'), (')', '@'), ('(', 'http'), (',', 'я'), ('ничего', 'не'), ('Как', 'же'), ('-', ')'), ('я', 'не'), ('Доброе', 'утро'), (':', 'DD'), ('не', '('), ('самом', 'деле'), ('amp', ';'), ('--', '--'), ('до', 'сих'), ('не', ')'), ('(', '!'), (',', 'чтобы'), (',', '!'), ('D', 'http'), ('=', ')'), ('об', 'этом'), ('что', 'я'), ('никто', 'не'), ('не', ':'), ('как', 'же'), ('и', '('), ('!', ','), (':', '!'), ('.', 'А'), ('?', '—'), ('с', 'кем'), (',', '.'), ('а', 'потом'), (':', '|'), ('Новый', 'Год'), ('.', ','), ('и', ')'), ('никогда', 'не'), (':', '.'), ('@', 'не'), ('.', 'Но'), ('не', '@'), ('#', 'євромайдан'), ('@', '!'), ('в', 'школе'), ('в', 'этом')]\n"
          ]
        }
      ],
      "source": [
        "bigrams = bigram_finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfjCYZa8TeX_"
      },
      "source": [
        "Как можно заметить, немаловажную роль в текстах занимает пунктуация."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJk1B39LVRP"
      },
      "source": [
        "## Стоп-слова и пунктуация\n",
        "\n",
        "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpWhsTuRLVRP",
        "outputId": "1fe147e8-8ca6-4f68-fe13-e932e112b8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "# у вас здесь, вероятно, выскочит ошибка и надо будет загрузить стоп слова (в тексте ошибки написано, как)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('russian'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OdRF7rlyLVRS",
        "outputId": "cbc67fd0-1e6f-47d0-9bb4-781a5d228a46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OfXiH98XLVRV"
      },
      "outputs": [],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtiIhHDMLVRY"
      },
      "source": [
        "В векторизаторах за стоп-слова, логичным образом, отвечает аргумент `stop_words`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZnbarm_LVRY",
        "outputId": "bbfd3930-6b7a-4e9c-aa1e-08ae3d4a35d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.76      0.78     29321\n",
            "    positive       0.76      0.80      0.78     27388\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize, stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr934O7yLVRb"
      },
      "source": [
        "Получилось чууть лучше. Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7O_oD1fLVRc"
      },
      "source": [
        "## Лемматизация\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – *лемме*. Почему это хорошо?\n",
        "* Во-первых, мы хотим рассматривать как отдельную фичу каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n",
        "\n",
        "### [Mystem](https://tech.yandex.ru/mystem/)\n",
        "Как с ним работать:\n",
        "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
        "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96HdoB7zLVRc",
        "outputId": "97957f4e-ec45-4598-8818-369eb935f342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-03 03:14:06--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.242, 5.45.205.243, 5.45.205.244, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.242|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cachev2-spb02.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz?lid=122 [following]\n",
            "--2022-06-03 03:14:07--  https://cachev2-spb02.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz?lid=122\n",
            "Resolving cachev2-spb02.cdn.yandex.net (cachev2-spb02.cdn.yandex.net)... 37.140.137.4, 2a02:6b8:0:2221::304\n",
            "Connecting to cachev2-spb02.cdn.yandex.net (cachev2-spb02.cdn.yandex.net)|37.140.137.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  12.9MB/s    in 1.2s    \n",
            "\n",
            "2022-06-03 03:14:09 (12.9 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzQwGwAaZWV5"
      },
      "outputs": [],
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w-_fkNtLVRf"
      },
      "source": [
        "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "* mystem_bin - путь к `mystem`, если их несколько\n",
        "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjHHLQv9txDq",
        "outputId": "a3023bb8-e858-4671-8e0e-af0589b314fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['какой', ' ', 'сегодня', ' ', 'хороший', ' ', 'день', ' ', 'для', ' ', 'прогулка', ' ', 'в', ' ', 'парк', '\\n']\n"
          ]
        }
      ],
      "source": [
        "example = 'Какой сегодня хороший день для прогулки в парке'\n",
        "print(mystem_analyzer.lemmatize(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1eftjkLVRi"
      },
      "source": [
        "А можно получить грамматическую информацию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4MLqlZnxNEj",
        "outputId": "6a9f0cd2-2afd-406d-ee5b-4ed0f5185641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'analysis': [{'gr': 'APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|вин,ед,муж,неод|им,ед,муж)',\n",
              "    'lex': 'какой',\n",
              "    'wt': 1}],\n",
              "  'text': 'Какой'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'ADV=', 'lex': 'сегодня', 'wt': 0.9987230301}],\n",
              "  'text': 'сегодня'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'A=(вин,ед,полн,муж,неод|им,ед,полн,муж)',\n",
              "    'lex': 'хороший',\n",
              "    'wt': 1}],\n",
              "  'text': 'хороший'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'S,муж,неод=(вин,ед|им,ед)',\n",
              "    'lex': 'день',\n",
              "    'wt': 0.9999201298}],\n",
              "  'text': 'день'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'PR=', 'lex': 'для', 'wt': 1}], 'text': 'для'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)',\n",
              "    'lex': 'прогулка',\n",
              "    'wt': 1}],\n",
              "  'text': 'прогулки'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'PR=', 'lex': 'в', 'wt': 0.9999917746}], 'text': 'в'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'gr': 'S,муж,неод=пр,ед', 'lex': 'парк', 'wt': 0.91102314}],\n",
              "  'text': 'парке'},\n",
              " {'text': '\\n'}]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mystem_analyzer.analyze(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADcGtz4JLVRl"
      },
      "source": [
        "Давайте терепь используем лемматизатор майстема в качестве токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x48Q56tiLVRn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def my_preproc(text):\n",
        "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
        "    text = mystem_analyzer.lemmatize(text)\n",
        "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEwOQTJPLVRq",
        "outputId": "460ab1e5-e97d-419f-e3b1-549af95accc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.74      0.76     28854\n",
            "    positive       0.74      0.77      0.76     27855\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJlvqWuALVRs"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHDkurN1zf7g",
        "outputId": "682919e3-c6da-4ee8-a63a-1c9b4005588a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SlwsLU7LVRt"
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaz0x7frLVRw"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdf6XoEbLVRw",
        "outputId": "0f82e381-33e0-4afa-9b98-51957bd8e691"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[3])\n",
        "ana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0KuHQGPgLVRz",
        "outputId": "6ba67d06-2df4-4c9f-8ead-dd291f03f2ac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'платить'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ana[0].normal_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFTkF8xUARlS"
      },
      "source": [
        "### [Natasha](https://github.com/natasha/)\n",
        "\n",
        "В библиотеке natasha реализовано множество полезных библиотек для русского языка: разбиение на токены и предложения, русскоязычные word embeddings, морфологический, синтаксический анализ, лемматизация, извлечение именованных сущностей и т.д. Модуль библиотеки Razdel, основанный на правилах, предназначен для разбиения текста на токены и предложения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CVeDxeIA6rg",
        "outputId": "4dd4ecad-4825-478f-8d99-4eb73b367709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOTkw9MpAnNN",
        "outputId": "5464ab4a-42a6-439d-9d72-f9d26dbf2452"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Substring(0, 13, 'Кружка-термос'),\n",
              " Substring(14, 16, 'на'),\n",
              " Substring(17, 20, '0.5'),\n",
              " Substring(20, 21, 'л'),\n",
              " Substring(22, 23, '('),\n",
              " Substring(23, 28, '50/64'),\n",
              " Substring(29, 32, 'см³'),\n",
              " Substring(32, 33, ','),\n",
              " Substring(34, 37, '516'),\n",
              " Substring(37, 38, ';'),\n",
              " Substring(38, 41, '...'),\n",
              " Substring(41, 42, ')')]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from razdel import tokenize\n",
        "\n",
        "tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftx-WzUbBCpO",
        "outputId": "55bd0e72-769d-4237-edc5-694005c3a02d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Кружка-термос',\n",
              " 'на',\n",
              " '0.5',\n",
              " 'л',\n",
              " '(',\n",
              " '50/64',\n",
              " 'см³',\n",
              " ',',\n",
              " '516',\n",
              " ';',\n",
              " '...',\n",
              " ')']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[_.text for _ in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyhsQp4MGbW8",
        "outputId": "4373bdf2-32fa-4fe6-8b9d-ce23de00e04d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 175 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.5.0)\n",
            "Collecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 131 kB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=ff6fce268ad2989a12b3b04a3cb96577508cd484b2b6cf561875d295e608a9d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 slovnet-0.5.0 yargy-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMO3jsqLKSIV"
      },
      "source": [
        "С помощью библиотеки natasha можно также лемматизировать тексты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJZgfRnvIS2q"
      },
      "outputs": [],
      "source": [
        "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "def natasha_lemmatize(text):\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "  return {_.text: _.lemma for _ in doc.tokens}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBtlnYlFBOKv",
        "outputId": "58f61325-ffd2-4fb1-beca-b1e269e38609"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'(': '(',\n",
              " ')': ')',\n",
              " ',': ',',\n",
              " '.': '.',\n",
              " '1': '1',\n",
              " '11': '11',\n",
              " '110-летия': '110-летие',\n",
              " '1909': '1909',\n",
              " '1909-1959': '1909-1959',\n",
              " '2010': '2010',\n",
              " '2019': '2019',\n",
              " 'Twitter': 'twitter',\n",
              " '«': '«',\n",
              " '»': '»',\n",
              " 'Бандера': 'бандера',\n",
              " 'Бандере': 'бандера',\n",
              " 'Бандеры': 'бандера',\n",
              " 'В': 'в',\n",
              " 'Верховной': 'верховный',\n",
              " 'Виктора': 'виктор',\n",
              " 'Героем': 'герой',\n",
              " 'Героя': 'герой',\n",
              " 'Житомирский': 'житомирский',\n",
              " 'Израиля': 'израиль',\n",
              " 'Йоэль': 'йоэль',\n",
              " 'Лион': 'лион',\n",
              " 'Львовский': 'львовский',\n",
              " 'Львовской': 'львовский',\n",
              " 'ОУН': 'оун',\n",
              " 'Организации': 'организация',\n",
              " 'Парламентарии': 'парламентарий',\n",
              " 'Петру': 'петр',\n",
              " 'Порошенко': 'порошенко',\n",
              " 'Посол': 'посол',\n",
              " 'Рады': 'рада',\n",
              " 'России': 'россия',\n",
              " 'Свое': 'свой',\n",
              " 'Степан': 'степан',\n",
              " 'Степана': 'степан',\n",
              " 'Украина': 'украина',\n",
              " 'Украине': 'украина',\n",
              " 'Украины': 'украина',\n",
              " 'Ющенко': 'ющенко',\n",
              " 'Я': 'я',\n",
              " 'а': 'а',\n",
              " 'аналогичное': 'аналогичный',\n",
              " 'антисемитизмом': 'антисемитизм',\n",
              " 'антисемитских': 'антисемитский',\n",
              " 'бороться': 'бороться',\n",
              " 'борьбе': 'борьба',\n",
              " 'был': 'быть',\n",
              " 'было': 'быть',\n",
              " 'в': 'в',\n",
              " 'вернуть': 'вернуть',\n",
              " 'властей': 'власть',\n",
              " 'впоследствии': 'впоследствии',\n",
              " 'выступающей': 'выступать',\n",
              " 'героем': 'герой',\n",
              " 'год': 'год',\n",
              " 'года': 'год',\n",
              " 'годом': 'год',\n",
              " 'году': 'год',\n",
              " 'государства': 'государство',\n",
              " 'декабря': 'декабрь',\n",
              " 'депутаты': 'депутат',\n",
              " 'деятельностью': 'деятельность',\n",
              " 'дипломат': 'дипломат',\n",
              " 'дня': 'день',\n",
              " 'должна': 'должный',\n",
              " 'евреев': 'еврей',\n",
              " 'за': 'за',\n",
              " 'забывать': 'забывать',\n",
              " 'запрещенной': 'запретить',\n",
              " 'заявление': 'заявление',\n",
              " 'звание': 'звание',\n",
              " 'и': 'и',\n",
              " 'из': 'из',\n",
              " 'информационном': 'информационный',\n",
              " 'исполнителей': 'исполнитель',\n",
              " 'их': 'они',\n",
              " 'июле': 'июль',\n",
              " 'к': 'к',\n",
              " 'как': 'как',\n",
              " 'ксенофобией': 'ксенофобия',\n",
              " 'кто': 'кто',\n",
              " 'лидера': 'лидер',\n",
              " 'лидеров': 'лидер',\n",
              " 'месяца': 'месяц',\n",
              " 'мифов': 'миф',\n",
              " 'могу': 'мочь',\n",
              " 'на': 'на',\n",
              " 'написал': 'написать',\n",
              " 'населением': 'население',\n",
              " 'националистов': 'националист',\n",
              " 'национальным': 'национальный',\n",
              " 'начале': 'начало',\n",
              " 'не': 'не',\n",
              " 'независимого': 'независимый',\n",
              " 'непосредственно': 'непосредственно',\n",
              " 'никоим': 'никой',\n",
              " 'о': 'о',\n",
              " 'области': 'область',\n",
              " 'областной': 'областной',\n",
              " 'образом': 'образ',\n",
              " 'обратились': 'обратиться',\n",
              " 'объявить': 'объявить',\n",
              " 'однако': 'однако',\n",
              " 'одним': 'один',\n",
              " 'он': 'он',\n",
              " 'остановит': 'остановить',\n",
              " 'отменено': 'отменить',\n",
              " 'отмечать': 'отмечать',\n",
              " 'период': 'период',\n",
              " 'подрывной': 'подрывной',\n",
              " 'поле': 'поле',\n",
              " 'помогает': 'помогать',\n",
              " 'поможет': 'помочь',\n",
              " 'понять': 'понять',\n",
              " 'посмертно': 'посмертно',\n",
              " 'почитание': 'почитание',\n",
              " 'празднованием': 'празднование',\n",
              " 'предложением': 'предложение',\n",
              " 'президентства': 'президентство',\n",
              " 'президенту': 'президент',\n",
              " 'преступлениях': 'преступление',\n",
              " 'признался': 'признаться',\n",
              " 'признан': 'признать',\n",
              " 'признание': 'признание',\n",
              " 'принимал': 'принимать',\n",
              " 'принял': 'принять',\n",
              " 'пришел': 'прийти',\n",
              " 'провозгласить': 'провозгласить',\n",
              " 'пропагандой': 'пропаганда',\n",
              " 'прославление': 'прославление',\n",
              " 'против': 'против',\n",
              " 'разместил': 'разместить',\n",
              " 'распространение': 'распространение',\n",
              " 'регионе': 'регион',\n",
              " 'решение': 'решение',\n",
              " 'решении': 'решение',\n",
              " 'родился': 'родиться',\n",
              " 'рождения': 'рождение',\n",
              " 'российской': 'российский',\n",
              " 'с': 'с',\n",
              " 'связи': 'связь',\n",
              " 'со': 'с',\n",
              " 'совершенных': 'совершить',\n",
              " 'совет': 'совет',\n",
              " 'создание': 'создание',\n",
              " 'созданных': 'создать',\n",
              " 'страны': 'страна',\n",
              " 'судом': 'суд',\n",
              " 'также': 'также',\n",
              " 'территориях': 'территория',\n",
              " 'тех': 'тот',\n",
              " 'уверены': 'уверить',\n",
              " 'ужасных': 'ужасный',\n",
              " 'узнав': 'узнать',\n",
              " 'украиноязычным': 'украиноязычный',\n",
              " 'украинских': 'украинский',\n",
              " 'участие': 'участие',\n",
              " 'через': 'через',\n",
              " 'что': 'что',\n",
              " 'шок': 'шок',\n",
              " 'это': 'это',\n",
              " 'января': 'январь',\n",
              " '—': '—'}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
        "\n",
        "natasha_lemmatize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rck5OVqhLVSA"
      },
      "source": [
        "### mystem vs. pymorphy vs. natasha\n",
        "\n",
        "1) *Мы надеемся, что вы пользуетесь линуксом*, но mystem работает невероятно медленно под windows на больших текстах.\n",
        "\n",
        "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту, natasha тоже с этим тоже не справляется успешно:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH2GQ4ddLVSB"
      },
      "outputs": [],
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwF-XsjeI3eX",
        "outputId": "37c2f021-9449-49a2-c78a-a60c6787f437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292578, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
            "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970059, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
          ]
        }
      ],
      "source": [
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9jezRVlFmDo",
        "outputId": "0ad463aa-647d-4b69-9f33-4bc55d44551b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'За': 'за', 'время': 'время', 'обучения': 'обучение', 'я': 'я', 'прослушал': 'прослушать', 'больше': 'большой', 'сорока': 'сорок', 'курсов': 'курс', '.': '.'}\n"
          ]
        }
      ],
      "source": [
        "print(natasha_lemmatize(homonym1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXjGBQPoI9gl",
        "outputId": "14f8b783-77e2-409d-8d0b-25296ec2d330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Сорока': 'сорок', 'своровала': 'своровать', 'блестящее': 'блестящий', 'украшение': 'украшение', 'со': 'с', 'стола': 'стол', '.': '.'}\n"
          ]
        }
      ],
      "source": [
        "print(natasha_lemmatize(homonym2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP5qFnilLVSI"
      },
      "source": [
        "## Словарь, закон Ципфа и закон Хипса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1umtd3OLVSI"
      },
      "source": [
        "Закон Ципфа -- эмпирическая закономерность: если все слова корпуса текста упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Иными словами, частотность слов убывает очень быстро."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY0cWJ7eLVSJ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7KqUEeFIrIH",
        "outputId": "b10d27cb-2750-4119-fb55-2c9131e9e681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import collocations \n",
        "nltk.download('genesis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zcOjFWwnJAfC",
        "outputId": "7fccef40-ea30-4654-de4b-e8af56b5e068"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIjqSVjpLVSL",
        "outputId": "99315ebc-9848-4ea6-c5c7-19bf67514eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2859142\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [token for tweet in df.text for token in word_tokenize(tweet) if token not in punctuation]\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oWC7NpkLVSO",
        "outputId": "0faf5178-a131-4301-eabd-53674341ac78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('не', 69267),\n",
              " ('и', 54916),\n",
              " ('в', 52853),\n",
              " ('я', 52506),\n",
              " ('RT', 38070),\n",
              " ('на', 35715),\n",
              " ('http', 32992),\n",
              " ('что', 31472),\n",
              " ('...', 28773),\n",
              " ('с', 27176)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "FrPkce0SLVSQ",
        "outputId": "bbd0244d-039b-41b5-b540-6b388a740226"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "first_100_freqs = [freq for word, freq in freq_dict_sorted[:100]]\n",
        "plt.plot(first_100_freqs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_N5V_K-LVSU"
      },
      "source": [
        "Закон Хипса -- обратная сторона закона Ципфа. Он описывает, что чем больше корпус, тем меньше новых слов добавляется с добавлением новых текстов. В какой-то момент корпус насыщается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0GieJSMU-O"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Построим модель на токенах с высокой частотой (>300 раз встречались в корпусе)"
      ],
      "metadata": {
        "id": "6CV8tMY-oFZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUQ6kAgPMqNn",
        "outputId": "3121e6ad-2048-424c-be9c-d6eb2c56478c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Есть', 305),\n",
              " ('волосы', 305),\n",
              " ('недавно', 304),\n",
              " ('родители', 304),\n",
              " ('л', 304),\n",
              " ('месяца', 304),\n",
              " ('хотелось', 303),\n",
              " ('вещи', 303),\n",
              " ('часть', 301),\n",
              " ('оч', 301)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "high_frec_tokens = [token for token in freq_dict_sorted if token[1] > 300]\n",
        "high_frec_tokens[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b8w8riQqJMC",
        "outputId": "16913a2b-9e12-4fcb-f06b-a92f8af57733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "836"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(high_frec_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpRVIlklrVeM",
        "outputId": "a1707b9c-99cb-466b-eaa2-c3b22a602913"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Есть',\n",
              " 'волосы',\n",
              " 'недавно',\n",
              " 'родители',\n",
              " 'л',\n",
              " 'месяца',\n",
              " 'хотелось',\n",
              " 'вещи',\n",
              " 'часть',\n",
              " 'оч']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_high_frec = [token[0] for token in high_frec_tokens]\n",
        "corpus_high_frec[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB_5_ZpeqeOI",
        "outputId": "5a349db2-9949-44b7-cc50-e52db80d705a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.65      0.66     29012\n",
            "    positive       0.65      0.67      0.66     27697\n",
            "\n",
            "    accuracy                           0.66     56709\n",
            "   macro avg       0.66      0.66      0.66     56709\n",
            "weighted avg       0.66      0.66      0.66     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1),  tokenizer=word_tokenize, stop_words=noise,\n",
        "                      vocabulary=corpus_high_frec)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим функцию, чтобы не дублировать код"
      ],
      "metadata": {
        "id": "ozjiveu8oR4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoZpngbVsV34"
      },
      "outputs": [],
      "source": [
        "def frec_comparing(token_list, train_corpus, train_label, test_corpus, test_labels):\n",
        "    vec = CountVectorizer(ngram_range=(1, 1),  tokenizer=word_tokenize, stop_words=noise,\n",
        "                          vocabulary=token_list)\n",
        "    bow = vec.fit_transform(train_corpus)\n",
        "    clf = LogisticRegression(random_state=42)\n",
        "    clf.fit(bow, train_label)\n",
        "    pred = clf.predict(vec.transform(test_corpus))\n",
        "    print(classification_report(pred, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Построим модель на токенах со средней частотой (от 50 до 300 раз встречались в корпусе)"
      ],
      "metadata": {
        "id": "f4F7W8qToqzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2XBOYRVtM9B",
        "outputId": "d80ed92e-d06a-47e0-a261-1e15ad085794"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['бухать',\n",
              " 'нормальных',\n",
              " 'картинкой',\n",
              " 'ужасные',\n",
              " 'сомневаюсь',\n",
              " 'опоздал',\n",
              " 'Dbnmjr',\n",
              " 'РИА',\n",
              " 'глючит',\n",
              " 'соболезнования']"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_medium_frec = [token[0] for token in freq_dict_sorted if ((token[1] > 50) & (token[1] <=300))]\n",
        "corpus_medium_frec[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYlB8jp0trd0",
        "outputId": "a6268b83-309e-4a67-e831-0ee5976d3045"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3619"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_medium_frec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvc03_7htIHh",
        "outputId": "3afe9610-d4fb-4a80-f196-999f676dc5ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.71      0.60      0.65     33158\n",
            "    positive       0.54      0.66      0.59     23551\n",
            "\n",
            "    accuracy                           0.62     56709\n",
            "   macro avg       0.62      0.63      0.62     56709\n",
            "weighted avg       0.64      0.62      0.63     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "frec_comparing(corpus_medium_frec, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Построим модель на токенах со средней частотой (<50 раз встречались в корпусе)"
      ],
      "metadata": {
        "id": "pHcd40Beo5Iu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3f1C8YBuU2N",
        "outputId": "d1f2f54f-1e3b-4cbf-c5e2-6d5555d72040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['линт',\n",
              " 'пхжааххв',\n",
              " '//t.co/QNODDQzuZ7',\n",
              " 'taaannyaaa',\n",
              " 'вправляет',\n",
              " '_Them__',\n",
              " 'LisaBeroud',\n",
              " 'приплатить',\n",
              " 'втащили',\n",
              " 'Лифта']"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_low_frec = [token[0] for token in freq_dict_sorted if token[1] <= 50]\n",
        "corpus_low_frec[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck48pI73ufus",
        "outputId": "522cae9b-c282-471e-e7e8-a00e9f355a6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "355166"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_low_frec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4r0mXi3ujIr",
        "outputId": "77a87afe-9de8-4c1c-a8d2-e6ff9311453e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.65      0.71     33276\n",
            "    positive       0.60      0.74      0.66     23433\n",
            "\n",
            "    accuracy                           0.69     56709\n",
            "   macro avg       0.69      0.70      0.69     56709\n",
            "weighted avg       0.71      0.69      0.69     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "frec_comparing(corpus_low_frec, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTB3DfuZu8lK"
      },
      "source": [
        "Исходя из результатов проведенного эксперимента, наибольшей важностью для классификации обладают наименее частотные слова."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV3fmzp-LVSU"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjkMxK9VLVSV",
        "outputId": "719fc2ca-24ae-45ec-fff4-8ae0ff369dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27829\n",
            "    positive       1.00      1.00      1.00     28880\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fRbUAvLVSX"
      },
      "source": [
        "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWG1rsaqeOgA"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним массив из токенов"
      ],
      "metadata": {
        "id": "U3vG26w6pFy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exi7xfxANeAj",
        "outputId": "730d5da1-db54-41e5-a8b1-ea2c7b2b3134"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "feature_names = vec.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим датасет с токенами и коэффициентами при них (возьмем коэффициенты по модулю)"
      ],
      "metadata": {
        "id": "wOe9n4onpEQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FdL-0_OQSpu1",
        "outputId": "01f74742-ea3c-48df-f9e3-ea9599a45230"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-10b77496-329a-4d71-ad0f-cba5c4274c84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!</td>\n",
              "      <td>0.018790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#</td>\n",
              "      <td>0.323677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>$</td>\n",
              "      <td>0.047157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>%</td>\n",
              "      <td>1.229323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>0.242602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266747</th>\n",
              "      <td>����но</td>\n",
              "      <td>0.000065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266748</th>\n",
              "      <td>������</td>\n",
              "      <td>0.007758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266749</th>\n",
              "      <td>�������</td>\n",
              "      <td>0.005434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266750</th>\n",
              "      <td>���������</td>\n",
              "      <td>0.008288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266751</th>\n",
              "      <td>������������</td>\n",
              "      <td>0.000653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>266752 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10b77496-329a-4d71-ad0f-cba5c4274c84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10b77496-329a-4d71-ad0f-cba5c4274c84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10b77496-329a-4d71-ad0f-cba5c4274c84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              tokens  importance\n",
              "0                  !    0.018790\n",
              "1                  #    0.323677\n",
              "2                  $    0.047157\n",
              "3                  %    1.229323\n",
              "4                  &    0.242602\n",
              "...              ...         ...\n",
              "266747        ����но    0.000065\n",
              "266748        ������    0.007758\n",
              "266749       �������    0.005434\n",
              "266750     ���������    0.008288\n",
              "266751  ������������    0.000653\n",
              "\n",
              "[266752 rows x 2 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_importance = pd.DataFrame(feature_names, columns=['tokens'])\n",
        "feature_importance['importance'] = np.abs(clf.coef_[0])\n",
        "\n",
        "feature_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отсортируем по величине коэффициента логистической регрессии при токене (коэффициент опредеделяет степень важноти токена для классификации). Вывделем топ-10 важных для классификации токенов."
      ],
      "metadata": {
        "id": "4iRDmxOvpVt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "B0it3iiWTCdC",
        "outputId": "b2d7f6a3-0e56-4100-af6d-cc59c67bf1fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7fbccab5-098d-4e30-8179-3885d8561e09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(</td>\n",
              "      <td>59.085625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>)</td>\n",
              "      <td>58.670303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44781</th>\n",
              "      <td>d</td>\n",
              "      <td>27.359225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>*</td>\n",
              "      <td>12.520973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102768</th>\n",
              "      <td>|</td>\n",
              "      <td>10.910412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46196</th>\n",
              "      <td>dd</td>\n",
              "      <td>10.837106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184202</th>\n",
              "      <td>о_о</td>\n",
              "      <td>10.427832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30737</th>\n",
              "      <td>^_^</td>\n",
              "      <td>9.113101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78836</th>\n",
              "      <td>o_o</td>\n",
              "      <td>8.555400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>-/</td>\n",
              "      <td>8.418278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fbccab5-098d-4e30-8179-3885d8561e09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fbccab5-098d-4e30-8179-3885d8561e09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fbccab5-098d-4e30-8179-3885d8561e09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       tokens  importance\n",
              "8           (   59.085625\n",
              "9           )   58.670303\n",
              "44781       d   27.359225\n",
              "10          *   12.520973\n",
              "102768      |   10.910412\n",
              "46196      dd   10.837106\n",
              "184202    о_о   10.427832\n",
              "30737     ^_^    9.113101\n",
              "78836     o_o    8.555400\n",
              "600        -/    8.418278"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_importance = feature_importance.sort_values(by=['importance'], ascending=False)\n",
        "feature_importance.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtAyItvLVSb"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqH07o-7LVSc",
        "outputId": "b9a3ba93-e49f-47a6-d8f0-e37594c92092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32717\n",
            "    positive       0.83      1.00      0.91     23992\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.92      0.93      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5THCOjMLVSg"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIUwDOabLVSh",
        "outputId": "fc5baa94-d748-4673-de88-287505d0ce86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      1.00      0.99     27709\n",
            "    positive       1.00      0.99      0.99     29000\n",
            "\n",
            "    accuracy                           0.99     56709\n",
            "   macro avg       0.99      0.99      0.99     56709\n",
            "weighted avg       0.99      0.99      0.99     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_E0uPpgLVSj"
      },
      "source": [
        "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
        "\n",
        "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0iHvqGAeOgB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-MQBYxveOgB"
      },
      "source": [
        "## Домашнее задание\n",
        "\n",
        "все материалы для выполения дз в `sem2.ipynb`\n",
        "\n",
        "\n",
        "### Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации.\n",
        "\n",
        "\n",
        "### Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их\n",
        "\n",
        "\n",
        "### Задание 3.\n",
        "\n",
        "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
        "\n",
        "2) подобрать оптимальный размер для hashing векторайзера \n",
        "\n",
        "3) убедиться что для сетки нет переобучения"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание 1 и Задание 2 уже выполнены выше"
      ],
      "metadata": {
        "id": "f2qf-HPhnufo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ниже выполним Задание 3"
      ],
      "metadata": {
        "id": "OfNw8xLPnz3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель на count-векторайзере"
      ],
      "metadata": {
        "id": "v1lX0qLtp6fR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ifvoo0nVeOgB",
        "outputId": "1dc1e1d2-5a1d-44e9-cd2f-4814af1003fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.76      0.78     29201\n",
            "    positive       0.76      0.80      0.78     27508\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)\n",
        "vec = CountVectorizer(analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель на tf-idf-векторайзере"
      ],
      "metadata": {
        "id": "Ty8qXMBHqDyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xtbc3GjeOgC",
        "outputId": "19e6c300-b700-4621-a4a6-aac28012193a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.77      0.77     27848\n",
            "    positive       0.78      0.78      0.78     28861\n",
            "\n",
            "    accuracy                           0.77     56709\n",
            "   macro avg       0.77      0.77      0.77     56709\n",
            "weighted avg       0.77      0.77      0.77     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель на Hashing-векторайзере"
      ],
      "metadata": {
        "id": "0ppywV4NqIFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим несколько моделей с разным значением гиперпараметра n_features, чтобы выбрать оптимальную модель"
      ],
      "metadata": {
        "id": "Z3LZ5YWFqQFd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKi0sx4Y67t",
        "outputId": "dd73b1e4-3440-4c88-c863-500ff394552a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.75      0.75     28361\n",
            "    positive       0.75      0.77      0.76     28348\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vec = HashingVectorizer(n_features=10**5, analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdRvuJikZmG6",
        "outputId": "b01645b1-5b6a-4d2f-e52b-0d03a2437c02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.76      0.76     28345\n",
            "    positive       0.76      0.77      0.77     28364\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vec = HashingVectorizer(n_features=10**6, analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7P7X0d3aOyV",
        "outputId": "5c5aaa92-f8c7-4207-c69c-90b69fa636f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.68      0.64      0.66     29459\n",
            "    positive       0.64      0.67      0.65     27250\n",
            "\n",
            "    accuracy                           0.66     56709\n",
            "   macro avg       0.66      0.66      0.66     56709\n",
            "weighted avg       0.66      0.66      0.66     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vec = HashingVectorizer(n_features=10**3, analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyvtNubyahCz",
        "outputId": "b84afa14-ed6d-4f25-b982-7b43080955a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.73      0.71      0.72     28459\n",
            "    positive       0.72      0.73      0.72     28250\n",
            "\n",
            "    accuracy                           0.72     56709\n",
            "   macro avg       0.72      0.72      0.72     56709\n",
            "weighted avg       0.72      0.72      0.72     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "vec = HashingVectorizer(n_features=10**4, analyzer='word', ngram_range=(1, 1), tokenizer = word_tokenize, \n",
        "                      stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Построим модель на полносвязной нейронной сети \n"
      ],
      "metadata": {
        "id": "gy9L9LF5qs7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменим лейблы на числовое представление"
      ],
      "metadata": {
        "id": "8syVvdeVrE9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZuIXSOkBhu9f"
      },
      "outputs": [],
      "source": [
        "y_train.replace({'positive': 1, 'negative': 0}, inplace=True)\n",
        "y_test.replace({'positive': 1, 'negative': 0}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8dlCVaCVazn_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "СОздадим датасеты и батчи данных"
      ],
      "metadata": {
        "id": "crM8O853rK0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XRn9X75OazqP"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FuRmJmCsazsi"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.batch(32)\n",
        "valid_data = valid_data.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель"
      ],
      "metadata": {
        "id": "F6WWgLttrQF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SJDd-mNyazv-"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gVs7TkjAcHI0"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    return input_data\n",
        "\n",
        "vocab_size = 50000\n",
        "seq_len = 100\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "e0qsWfsMdSIB"
      },
      "outputs": [],
      "source": [
        "embedding_dim=200\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, embedding_dim, input_length=seq_len),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wDo09DPWdW8C"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFlfYDyVdfJJ",
        "outputId": "3160108d-3487-474d-cb97-52251974ced6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5317/5317 [==============================] - 818s 154ms/step - loss: 0.3383 - accuracy: 0.8287 - val_loss: 0.3003 - val_accuracy: 0.8476\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc16bf58590>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model.fit(train_data, validation_data=valid_data, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим classification_report"
      ],
      "metadata": {
        "id": "b6W7NDVKrUAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(valid_data)"
      ],
      "metadata": {
        "id": "F7e1q2yoQl_l"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_2 = []\n",
        "for i in pred:\n",
        "    if i>=0.5:\n",
        "        pred_2.append(1)\n",
        "    elif i<0.5:\n",
        "        pred_2.append(0)\n",
        "print(classification_report(pred_2, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnQ-QOwcarm_",
        "outputId": "17a09aa2-309e-4ba7-8ad8-5ab7efe60956"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.82      0.85     29887\n",
            "           1       0.82      0.88      0.84     26822\n",
            "\n",
            "    accuracy                           0.85     56709\n",
            "   macro avg       0.85      0.85      0.85     56709\n",
            "weighted avg       0.85      0.85      0.85     56709\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw2_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}